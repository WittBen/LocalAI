LocalChatBot â€“ WPF Client for LocalAI

This project is a WPF application in C# that provides a chat interface for a locally running AI model.
It connects to a Foundry-compatible LocalAI backend (OpenAI API compatible) using HttpClient with streaming support.

âœ¨ Features

  Modern chat interface with input box and message history
  Connection to a locally hosted model (e.g. gpt-oss-20b-cuda-gpu)
  Streaming support â€“ responses are displayed while they are generated
  Clean MVVM architecture for the WPF UI
  Easy to extend with multiple models, RAG integration, or custom data sources

ðŸ›  Requirements

  .NET 8 SDK (or later)
  Visual Studio 2022 / JetBrains Rider
  A running LocalAI/Foundry model (OpenAI-compatible REST API)

Example:

  local-ai serve --model gpt-oss-20b-cuda-gpu

Windows 10/11

ðŸš€ Getting Started

  Clone the repository:

  git clone https://github.com/YOURUSERNAME/LocalChatBot.git
  cd LocalChatBot

  Configure the API endpoint in App.config or in code:

  <add key="ApiBaseUrl" value="http://localhost:8080/v1/chat/completions" />

Run the application:

  dotnet run --project LocalChatBot

ðŸ’¬ Example

  Input â†’
  Hi, can you briefly explain how RAG works?


  Streaming Response â†’
  RAG (Retrieval-Augmented Generation) combines ...

ðŸ“‚ Project Structure
LocalChatBot/
â”‚â”€â”€ LocalChatBot.csproj       # Project file
â”‚â”€â”€ MainWindow.xaml           # Chat UI (XAML)
â”‚â”€â”€ MainWindow.xaml.cs        # Code-behind
â”‚â”€â”€ Services/                 # API client + streaming
â”‚â”€â”€ Models/                   # Data models for chat
â”‚â”€â”€ ViewModels/               # MVVM logic
â”‚â”€â”€ docs/                     # Screenshots, documentation

